{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    print(\"TensorboardX not available\")\n",
    "    pass\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating text\n",
    "\n",
    "How do you do this? There are many difficulties with different sentence lengths, different grammar or contextual information. In this notebook we will cover how to do this using sequence to sequence learning.\n",
    "\n",
    "![](img/hello-lead.png)\n",
    "\n",
    "## Sequence to sequence learning\n",
    "We will use pytorch to translate short sentences from French to English and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the needed data\n",
    "if not os.path.isfile('data.zip'):\n",
    "    ! curl -o data.zip https://download.pytorch.org/tutorial/data.zip && unzip data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de question !\n",
      "Really?\tVraiment ?\n",
      "Really?\tVrai ?\n",
      "Really?\tAh bon ?\n",
      "Thanks.\tMerci !\n",
      "We try.\tOn essaye.\n",
      "We won.\tNous avons gagné.\n",
      "We won.\tNous gagnâmes.\n",
      "We won.\tNous l'avons emporté.\n",
      "We won.\tNous l'empor\n"
     ]
    }
   ],
   "source": [
    "# Take a quick view of the data.\n",
    "with open('data/eng-fra.txt') as f:\n",
    "    f.seek(1000)\n",
    "    print(f.read(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data I\n",
    "\n",
    "* Create a Language class that maps indexes to words and words to indexes\n",
    "\n",
    "**indexes to word**\n",
    "```python\n",
    "{0: SOS,\n",
    " 1: EOS,\n",
    " 2: The\n",
    " ...\n",
    " n: World\n",
    "}\n",
    "```\n",
    "\n",
    "**words to indexes**\n",
    "```python\n",
    "{SOS: 0,\n",
    " EOS: 1,\n",
    " The: 2\n",
    " ...\n",
    " World: n\n",
    "}\n",
    "```\n",
    "\n",
    "* Implement functions to convert the letters to ASCII and remove rare letters. (á, ò, ê -> a, o, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    \"\"\" Utility class that serves as a language dictionary \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # Count how often a word occurs in the language data.\n",
    "        self.word2count = {}\n",
    "        # Words are mapped to indices and vice versa\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2index = {v:k for k, v in self.index2word.items()}\n",
    "        # Total word count\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\" Process words in a sentence string. \"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\" Process a word (e.g. put it in vocabulary and count) \"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        elif word != 'SOS' and word != 'EOS':\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def translate_indexes(self, idx):\n",
    "        \"\"\" Takes in a vector of indices and returns the sentence. \"\"\"\n",
    "        return [self.index2word[i] for i in idx]\n",
    "    \n",
    "    def translate_words(self, words):\n",
    "        \"\"\" Takes in a vector of indices and returns the sentence. \"\"\"\n",
    "        return [self.word2index[w] for w in words.split(' ')]\n",
    "    \n",
    "    \n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicode2ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode2ascii(s.lower().strip())\n",
    "    s = re.sub(r\"\\s?[.!?]\", r\" EOS\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_langs(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    input_lang = Language(lang1)\n",
    "    output_lang = Language(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data II\n",
    "Since there are a lot of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. \n",
    "Here the maximum length is 10 words (that includes ending punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. \n",
    "(accounting for apostrophes replaced earlier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs):\n",
    "    MAX_LENGTH = 10\n",
    "    \n",
    "    eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    "    )\n",
    "    \n",
    "    def filter_pair(p):\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH \\\n",
    "            and p[0].startswith(eng_prefixes)\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data III\n",
    "\n",
    "Read the data from the text files, normalize the sentences, create the Language instances from the Language class and wrap the two languages in a Data class so we can shuffle the sentences and query them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2922\n",
      "fra 4486\n",
      "First data pair: ['you re jealous EOS' 'tu es jalouse EOS']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, pairs, lang_1, lang_2):\n",
    "        self.pairs = np.array(pairs)        \n",
    "        np.random.shuffle(self.pairs)\n",
    "        idx_1 = [[lang_1.word2index[word] for word in s.split(' ')] \n",
    "                               for s in self.pairs[:, 0]]\n",
    "        idx_2 = [[lang_2.word2index[word] for word in s.split(' ')]\n",
    "                               for s in self.pairs[:, 1]]\n",
    "        self.idx_pairs = np.array(list(zip(idx_1, idx_2)))\n",
    "        self.shuffle_idx = np.arange(len(pairs))\n",
    "                \n",
    "    def __str__(self):\n",
    "        return(self.pairs)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.shuffle_idx)\n",
    "        self.pairs = self.pairs[self.shuffle_idx]\n",
    "        self.idx_pairs = self.idx_pairs[self.shuffle_idx]      \n",
    "    \n",
    "def prepare_data(lang1, lang2):\n",
    "    # read_langs initialized the Language objects (still empty) and returns the pair sentences.\n",
    "    input_lang, output_lang, pairs = read_langs(lang1, lang2)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    \n",
    "    # Reduce data. We haven't got all day to train a model.\n",
    "    pairs = filter_pairs(pairs) \n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting words...\")\n",
    "    \n",
    "    # Process the language pairs.\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, Data(pairs, input_lang, output_lang)\n",
    "\n",
    "\n",
    "eng, fra, data = prepare_data('eng', 'fra')\n",
    "print(f\"First data pair: {data.pairs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence model overview\n",
    "\n",
    "![](img/seq2seq.png)\n",
    "\n",
    "## The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word. Every output could be seen as the context of the sentence up to that point.\n",
    "\n",
    "<img src=\"img/training_seq2seq_many2may.svg\" alt=\"drawing\" style=\"height:300px;float: left;\"/>\n",
    "\n",
    "![](img/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ..., device=device):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        self.device = device\n",
    "        if device == 'cuda':\n",
    "            self.cuda()\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        # Embed words\n",
    "\n",
    "        # make shape (seq_length, batch_size, input_size)\n",
    "        \n",
    "        # init hidden layer with start of sequence --> SOS\n",
    "\n",
    "        #\n",
    "\n",
    "        return x, h\n",
    "        \n",
    "m = Encoder(..., device=device)\n",
    "\n",
    "eng_sentence = data.pairs[0][0]\n",
    "print(f'Test sentence: {eng_sentence}')\n",
    "sentence = torch.tensor(eng.translate_words(eng_sentence), device=device)\n",
    "print(f'Test tensor: {sentence}')\n",
    "enc_out, enc_hidden = m(sentence)\n",
    "print(f'output shape: {enc_out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Decoder\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state).\n",
    "    \n",
    "![](img/decoder-network-adapted.png)\n",
    "    \n",
    "\n",
    "The power of this model lies in the fact that it can map sequences of different lengths to each other. As you can see the inputs and outputs are not correlated and their lengths can differ. This opens a whole new range of problems which can now be solved using such architecture.    \n",
    "    \n",
    "<img src=\"img/unfolded-encoder-decoder.png\" alt=\"drawing\" style=\"width:500px;float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ..., device=device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = 'simple'\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        self.device = device\n",
    "        if device == 'cuda':\n",
    "            self.cuda()\n",
    "            \n",
    "    def forward(self, word, h):\n",
    "        # Make embedding of word\n",
    "        \n",
    "        # Map from shape (seq_len, embedding_size) to (seq_len, batch, embedding_size)\n",
    "        \n",
    "        return out, h\n",
    "    \n",
    "m = Decoder(..., device=device)\n",
    "m.train(False)\n",
    "out, hidden = m(torch.tensor([1]) ,torch.zeros(1, 1, 20))\n",
    "out.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is wrong with the simple decoder?\n",
    "\n",
    "![](img/seq2seq.png)\n",
    "![](img/vanishing_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Attention\n",
    "![](img/attention-decoder-network-adapted.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, ..., device=device):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.decoder = 'attention'\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        self.device = device\n",
    "        if device == 'cuda':\n",
    "            self.cuda()\n",
    "        \n",
    "    def forward(self, word, h, encoder_outputs):\n",
    "        # Make embedding of word\n",
    "\n",
    "        # Map from shape (seq_len, embedding_size) to (seq_len, batch, embedding_size)\n",
    "        \n",
    "        # Concatenate the word embedding and the last hidden state, so that attention weights can be determined.\n",
    "        \n",
    "        # attention applied\n",
    "        \n",
    "        # Combine attention weights with encoder outputs\n",
    "   \n",
    "        # Combine attention with input\n",
    "\n",
    "        # Generate sequence\n",
    "        \n",
    "        return x, h\n",
    "\n",
    "\n",
    "# define settings\n",
    "embedding_size = 256\n",
    "hidden_size    = 256\n",
    "max_length     = 10\n",
    "\n",
    "# Encode a test sentence\n",
    "enc          = Encoder(..., device=device)\n",
    "eng_sentence = data.pairs[0][0]\n",
    "sentence = torch.tensor(eng.translate_words(eng_sentence), device=device)\n",
    "out, h   = enc(sentence)\n",
    "print(\"out.shape:\", out.shape)\n",
    "\n",
    "# in case sentence is shorter than max_length, pad with zeros\n",
    "encoder_outputs = ...\n",
    "\n",
    "# Create decoder\n",
    "dec = AttentionDecoder(..., device=device)\n",
    "dec(torch.tensor([1], device=device), h, encoder_outputs)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function to run the decoder & calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decoder(decoder, criterion, sentence, h, teacher_forcing=False, encoder_outputs=None):\n",
    "    loss = 0\n",
    "    word = torch.tensor([0], device=device) # <SOS>\n",
    "    for j in range(sentence.shape[0]):\n",
    "        if decoder.decoder == 'attention':\n",
    "            x, h = decoder(word, h, encoder_outputs)\n",
    "        else:\n",
    "            x, h = decoder(word, h)\n",
    "\n",
    "        loss += criterion(x.view(1, -1), sentence[j].view(-1))\n",
    "        if teacher_forcing:\n",
    "            word = sentence[j]\n",
    "        else:\n",
    "            word = x.argmax().detach()\n",
    "        if word.item() == 1: # <EOS>\n",
    "            break\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs                = 10\n",
    "teacher_forcing_ratio = 0.5\n",
    "embedding_size        = 100\n",
    "context_vector_size   = 256\n",
    "\n",
    "encoder = Encoder(...)\n",
    "decoder = AttentionDecoder(...)\n",
    "\n",
    "if 'SummaryWriter' in globals():\n",
    "    writer = SummaryWriter('tb/train-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelf bouwen\n",
    "def train(encoder, decoder):\n",
    "    # Criterion\n",
    "    \n",
    "    # Optimizers\n",
    "    \n",
    "    # Models\n",
    "\n",
    "    # Train loop\n",
    "\n",
    "        # Encode the input language\n",
    "            \n",
    "        # pad encoder outputs with zeros\n",
    "            \n",
    "        # implement teacher_forcing\n",
    "        \n",
    "        # print something\n",
    "        \n",
    "        \n",
    "train(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(eng.n_words, embedding_size, context_vector_size)\n",
    "# encoder.load_state_dict(torch.load('models/encoder_10_epochs.pt', map_location=device))\n",
    "\n",
    "# decoder = AttentionDecoder(embedding_size, context_vector_size, fra.n_words)\n",
    "# decoder.load_state_dict(torch.load('models/decoder_10_epochs.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start translating some sentences from English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(start, end):\n",
    "    for i in range(start, end):\n",
    "        pair = data.idx_pairs[i]\n",
    "        eng_sentence = torch.tensor(pair[0], device=device)\n",
    "        fra_sentence = torch.tensor(pair[1], device=device)\n",
    "\n",
    "        print('English sentence:\\t', ' '.join([eng.index2word[i.item()] for i in eng_sentence[:-1]]))\n",
    "        print('French sentence:\\t', ' '.join([fra.index2word[i.item()] for i in fra_sentence[:-1]]))\n",
    "\n",
    "        # Encode the input language\n",
    "        out, h = encoder(eng_sentence)        \n",
    "        encoder_outputs = torch.zeros(max_length, out.shape[-1], device=device)\n",
    "        encoder_outputs[:out.shape[0], :out.shape[-1]] = out.view(out.shape[0], -1)\n",
    "        \n",
    "        word = torch.tensor([0], device=device) # <SOS>\n",
    "  \n",
    "        translation = []\n",
    "        for j in range(eng_sentence.shape[0]):\n",
    "            x, h = decoder(word, h, encoder_outputs=encoder_outputs)\n",
    "  \n",
    "            word = x.argmax().detach()\n",
    "            translation.append(word.cpu().data.tolist())\n",
    "\n",
    "            if word.item() == 1: # <EOS>\n",
    "                break\n",
    "        print('\\nModel translation:\\t', ' '.join([fra.index2word[i] for i in translation][:-1]), '\\n' + '-'*50)\n",
    "        \n",
    "translate(0, 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
