{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    print(\"TensorboardX not available\")\n",
    "    pass\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating text\n",
    "\n",
    "How do you do this? There are many difficulties with different sentence lengths, different grammar or contextual information. In this notebook we will cover how to do this using sequence to sequence learning.\n",
    "\n",
    "![](img/hello-lead.png)\n",
    "\n",
    "## Sequence to sequence learning\n",
    "We will use pytorch to translate short sentences from French to English and vice versa\n",
    "\n",
    "Some concepts that will be covered:\n",
    "- Embeddings\n",
    "- Recurrent neural networks\n",
    "- Encoder / decoders\n",
    "- Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the needed data\n",
    "if not os.path.isfile('data.zip'):\n",
    "    ! curl -o data.zip https://download.pytorch.org/tutorial/data.zip && unzip data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " de question !\n",
      "Really?\tVraiment ?\n",
      "Really?\tVrai ?\n",
      "Really?\tAh bon ?\n",
      "Thanks.\tMerci !\n",
      "We try.\tOn essaye.\n",
      "We won.\tNous avons gagné.\n",
      "We won.\tNous gagnâmes.\n",
      "We won.\tNous l'avons emporté.\n",
      "We won.\tNous l'empor\n"
     ]
    }
   ],
   "source": [
    "# Take a quick view of the data.\n",
    "with open('data/eng-fra.txt') as f:\n",
    "    f.seek(1000)\n",
    "    print(f.read(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data 0\n",
    "During the process, we need to interact with the languages quite often. We probably need to switch between words and indexes & vice versa. Therefore we need to keep some sort of mapping between the two. Something like:\n",
    "\n",
    "**indexes to word**\n",
    "```python\n",
    "{0: 'SOS',\n",
    " 1: 'EOS',\n",
    " 2: 'The'\n",
    " ...\n",
    " n: 'World'\n",
    "}\n",
    "```\n",
    "\n",
    "**words to indexes**\n",
    "```python\n",
    "{'SOS': 0,\n",
    " 'EOS': 1,\n",
    " 'The': 2\n",
    " ...\n",
    " 'World': n\n",
    "}\n",
    "```\n",
    "\n",
    "A nice way to do this, is creating an object that stores these mappings. This is already done for you. To check, go to: `utils.Language`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data 1\n",
    "\n",
    "What should we do?\n",
    "- Reading data from file\n",
    "- Make lowercase\n",
    "- Remove non-letter characters\n",
    "- Mark the end of the scentence\n",
    "- Mark the start of the scentence\n",
    "- Remove rare letters. (á, ò, ê)\n",
    "- ...\n",
    "- Translate words into numbers?\n",
    "\n",
    "This is already done for you. To check, go to: `preprocessing.normalize_string`, `preprocessing.unicode2ascii` and `preprocessing.read_lang_pairs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data 2\n",
    "Since there are a lot of example sentences and we want to train something quickly in this short training, we'll trim the dataset to only contain relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (assuming that apostrophes are replaced earlier).\n",
    "\n",
    "In short:\n",
    "- only sentences < 10 words\n",
    "- only sentences that start with 'I am', 'He is' etc.\n",
    "\n",
    "This function is already created. To check it out, go to: `preprocessing.filter_pairs_eng2other`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data 3\n",
    "\n",
    "Next to this, it would be nice to create an object that contains the data. This object can help with several tasks, such as querying the data or shuffling the sentences. Something we need later on in the training process.\n",
    "\n",
    "We also need to:\n",
    "- Create a `Data` class\n",
    "\n",
    "This is already done for you. To check, go to: `utils.Data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data 4\n",
    "\n",
    "Now we have to tie it all together. We need to:\n",
    "- Initialize the `Language` objects\n",
    "- Preprocess the sentence pairs\n",
    "- Filter out simple cases for this training\n",
    "\n",
    "We can of course put this in our `preprocessing` module as well, but for illustration purposes, we've put it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Language, Data\n",
    "from preprocessing import read_lang_pairs, filter_pairs_eng2other\n",
    "\n",
    "\n",
    "def prepare_dataset(from_lang, to_lang):\n",
    "    \"\"\" Initializes the Language objects (still empty), creates the sentences pairs\n",
    "    and returns a Data object containing all languages and scentence pairs.\n",
    "    \"\"\"\n",
    "    pairs = read_lang_pairs(from_lang, to_lang)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    \n",
    "    # Reduce data. We haven't got all day to train a model.\n",
    "    if from_lang != 'eng':\n",
    "         raise ValueError(f'No filter implemented for translation from {from_lang} to {to_lang}')\n",
    "    \n",
    "    pairs = filter_pairs_eng2other(pairs) \n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    \n",
    "    input_lang = Language(from_lang)\n",
    "    output_lang = Language(to_lang)\n",
    "    # Add pairs to the languages\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, Data(pairs, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counted words:\n",
      "eng 2922\n",
      "fra 4486\n",
      "First data pair: ['i m EOS' 'j ai ans EOS']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "eng, fra, data = prepare_dataset('eng', 'fra')\n",
    "print(f\"First data pair: {data.pairs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence model overview\n",
    "So this is what we're going to build:\n",
    "\n",
    "![](img/seq2seq.png)\n",
    "\n",
    "Looking at the statistics printed above (of our simplified dataset), do you see any interesting output?\n",
    "- More French words than English\n",
    "- Quite a lot of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word. Every output could be seen as the context of the sentence up to that point.\n",
    "\n",
    "<img src=\"img/training_seq2seq_many2may.svg\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "As mentioned above, we have quite some words in our dictionaries. Therefore, it might be a good idea to create embeddings of our words since we're only passing context anyway.\n",
    "\n",
    "![](img/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_words, embedding_size, hidden_size, device=device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # The word embeddings will also be trained simultaneously with the NN weights\n",
    "        # To freeze them --> m.embedding.weight.requires_grad = False\n",
    "        self.embedding = nn.Embedding(n_words, embedding_size)  \n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size)\n",
    "        \n",
    "        self.device = device\n",
    "        if device == 'cuda':\n",
    "            self.cuda()\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        # shape (seq_length, batch_size, input_size)\n",
    "        dense_vector = self.embedding(x).view(x.shape[0], 1, -1)\n",
    "        \n",
    "        # init hidden layer at beginning of sequence --> SOS\n",
    "        h = torch.zeros(1, 1, self.hidden_size, device=self.device)\n",
    "        \n",
    "        x, h = self.rnn(dense_vector, h)\n",
    "\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: 'i m EOS'\n",
      "Test tensor  : tensor([2, 3, 1])\n",
      "output shape : torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_words': eng.n_words,\n",
    "    'embedding_size': 10,\n",
    "    'hidden_size': 2,\n",
    "    'device': device\n",
    "}        \n",
    "\n",
    "m = Encoder(**params)\n",
    "\n",
    "eng_sentence = data.pairs[0][0]\n",
    "sentence = torch.tensor(eng.translate_words(eng_sentence), device=device)\n",
    "enc_out, enc_hidden = m(sentence)\n",
    "\n",
    "print(f\"Test sentence: '{eng_sentence}'\")\n",
    "print(f\"Test tensor  : {sentence}\")\n",
    "print(f\"output shape : {enc_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Decoder\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state).\n",
    "    \n",
    "![](img/decoder-network-adapted.png)\n",
    "    \n",
    "\n",
    "The power of this model lies in the fact that it can map sequences of different lengths to each other. As you can see the inputs and outputs are not correlated and their lengths can differ. This opens a whole new range of problems which can now be solved using such architecture.    \n",
    "    \n",
    "<img src=\"img/unfolded-encoder-decoder.png\" alt=\"drawing\" style=\"width:500px;float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 2922]), torch.Size([1, 1, 20]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, device=device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = 'simple'\n",
    "        self.hidden_size = hidden_size\n",
    "        # Lookup table for the last word activation.\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "        self.device = device\n",
    "        if device == 'cuda':\n",
    "            self.cuda()\n",
    "            \n",
    "    def forward(self, word, h):\n",
    "        \"\"\" Forward pass of the NN\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        word : torch.tensor\n",
    "            Last word or start of sentence token.\n",
    "        h : torch.tensor\n",
    "            Hidden state or context tensor.\n",
    "        \"\"\"\n",
    "        # Map from shape (seq_len, embedding_size) to (seq_len, batch, embedding_size)\n",
    "        # Note: seq_len is the number of words in the sentence\n",
    "        word_embedding = self.embedding(word).view(1, 1, -1)\n",
    "        x, h = self.rnn(word_embedding, h)\n",
    "\n",
    "        return self.out(x), h\n",
    "\n",
    "params = {\n",
    "    'embedding_size': 10,\n",
    "    'hidden_size': 20,\n",
    "    'output_size': end.n_words,\n",
    "    'device': device\n",
    "}  \n",
    "m = Decoder(**params)\n",
    "m.train(False)\n",
    "out, hidden = m(torch.tensor([1]) ,torch.zeros(1, 1, 20))\n",
    "out.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is wrong with the simple decoder?\n",
    "\n",
    "![](img/seq2seq.png)\n",
    "![](img/vanishing_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Attention\n",
    "<img src=\"img/seq2seq-attn.png\" alt=\"drawing\" style=\"height:400px;\"/>\n",
    "\n",
    "![](img/attention-decoder-network-adapted.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, dropout=0.1, max_length=10, device=device):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.decoder = 'attention'\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(output_size, embedding_size),\n",
    "        )\n",
    "        \n",
    "        # Seperate neural network to learn the attention weights\n",
    "        self.attention_weights = nn.Sequential(\n",
    "            nn.Linear(embedding_size + hidden_size, max_length),\n",
    "            nn.Softmax(2)\n",
    "        )\n",
    "        self.attention_combine = nn.Sequential(\n",
    "            nn.Linear(hidden_size + embedding_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            self.cuda()\n",
    "        \n",
    "    def forward(self, word, h, encoder_outputs):\n",
    "        \"\"\"\n",
    "        :param word: (LongTensor) The word indices. This is the last activated word or \n",
    "        :param h: (tensor) The hidden state from the previous step. In the first step, the hidden state of the encoder.\n",
    "        :param encoder_outputs: (tensor) Zero padded (max_length, shape, shape) outputs from the encoder.\n",
    "        \"\"\"\n",
    "        # map from shape (seq_len, embedding_size) to (seq_len, batch, embedding_size) \n",
    "        # Note: seq length is the number of words in the sentence\n",
    "        word_embedding = self.embedding(word).view(1, 1, -1)\n",
    "        # Concatenate the word embedding and the last hidden state, so that attention weights can be determined.\n",
    "        x = torch.cat([word_embedding, h], dim=2)\n",
    "        \n",
    "        # attention applied\n",
    "        attention_weights = self.attention_weights(x)\n",
    "        \n",
    "        x = torch.bmm(attention_weights, encoder_outputs.unsqueeze(0))  # could also be done with matmul\n",
    "   \n",
    "        # attention combined\n",
    "        x = torch.cat((word_embedding, x), 2)\n",
    "        x = self.attention_combine(x)\n",
    "        \n",
    "        x, h = self.rnn(x, h)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([3, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_words': eng.n_words,\n",
    "    'embedding_size': 256,\n",
    "    'hidden_size': 256,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "enc      = Encoder(**params)\n",
    "sentence = torch.tensor([1, 23, 9], device=device)\n",
    "out, h   = enc(sentence)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs.shape: torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# in case sentence is shorter than max_length, pad with zeros\n",
    "max_length = 10\n",
    "encoder_outputs = torch.zeros(max_length, out.shape[-1], device=device)\n",
    "encoder_outputs[:out.shape[0], :out.shape[-1]] = out.view(out.shape[0], -1)\n",
    "print(f'encoder_outputs.shape: {encoder_outputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'embedding_size': 256,\n",
    "    'hidden_size': 256,\n",
    "    'output_size': 2,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "a_dec = AttentionDecoder(**params)\n",
    "a_dec(torch.tensor([1], device=device), h, encoder_outputs)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function to run the decoder & calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decoder(decoder, criterion, sentence, h, teacher_forcing=False, encoder_outputs=None):\n",
    "    loss = 0\n",
    "    word = torch.tensor([0], device=device) # <SOS>\n",
    "    for j in range(sentence.shape[0]):\n",
    "        if decoder.decoder == 'attention':\n",
    "            x, h = decoder(word, h, encoder_outputs)\n",
    "        else:\n",
    "            x, h = decoder(word, h)\n",
    "\n",
    "        loss += criterion(x.view(1, -1), sentence[j].view(-1))\n",
    "        if teacher_forcing:\n",
    "            word = sentence[j]\n",
    "        else:\n",
    "            word = x.argmax().detach()\n",
    "        if word.item() == 1: # <EOS>\n",
    "            break\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size        = 100\n",
    "context_vector_size   = 256\n",
    "\n",
    "enc_params = {\n",
    "    'n_words': eng.n_words,\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': context_vector_size,\n",
    "    'device': device\n",
    "}\n",
    "encoder = Encoder(**enc_params)\n",
    "\n",
    "dec_params = {\n",
    "    'embedding_size': embedding_size,\n",
    "    'hidden_size': context_vector_size,\n",
    "    'output_size': fra.n_words,\n",
    "    'device': device\n",
    "}\n",
    "decoder = AttentionDecoder(**dec_params)\n",
    "\n",
    "if 'SummaryWriter' in globals():\n",
    "    writer = SummaryWriter('tb/train-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs                = 10\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(encoder, decoder):\n",
    "    # Criterion\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # Optimizers\n",
    "    optim_encoder = torch.optim.SGD(encoder.parameters(), lr=0.01)\n",
    "    optim_decoder = torch.optim.SGD(decoder.parameters(), lr=0.01)  \n",
    "    \n",
    "    # Models\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(epochs):\n",
    "        data.shuffle()\n",
    "        for i in range(data.pairs.shape[0]):\n",
    "            optim_decoder.zero_grad()\n",
    "            optim_encoder.zero_grad()\n",
    "            \n",
    "            pair = data.idx_pairs[i]\n",
    "            eng_sentence = torch.tensor(pair[0], device=device)\n",
    "            fra_sentence = torch.tensor(pair[1], device=device)\n",
    "\n",
    "            # Encode the input language\n",
    "            out, h = encoder(eng_sentence)        \n",
    "            \n",
    "            # pad encoder outputs with zeros\n",
    "            encoder_outputs = torch.zeros(max_length, out.shape[-1], device=device)\n",
    "            if decoder.decoder == 'attention':\n",
    "                encoder_outputs[:out.shape[0], :out.shape[-1]] = out.view(out.shape[0], -1) # remove batch dim\n",
    "            \n",
    "            # implement teacher_forcing\n",
    "            teacher_forcing = np.random.rand() < teacher_forcing_ratio\n",
    "            loss = run_decoder(decoder, criterion, fra_sentence, h, teacher_forcing, encoder_outputs)\n",
    "            loss.backward()\n",
    "            \n",
    "            if 'SummaryWriter' in globals():\n",
    "                writer.add_scalar('loss', loss.cpu().item() / (len(fra_sentence)))\n",
    "\n",
    "            optim_decoder.step()\n",
    "            optim_encoder.step()\n",
    "\n",
    "        print(f'epoch {epoch}')\n",
    "\n",
    "train(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(eng.n_words, embedding_size, context_vector_size)\n",
    "encoder.load_state_dict(torch.load('models/encoder_10_epochs.pt', map_location=device))\n",
    "\n",
    "decoder = AttentionDecoder(embedding_size, context_vector_size, fra.n_words)\n",
    "decoder.load_state_dict(torch.load('models/decoder_10_epochs.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start translating some sentences from English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:\t you re jealous\n",
      "French sentence:\t tu es jalouse\n",
      "\n",
      "Model translation:\t vous etes jalouse \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m not your husband anymore\n",
      "French sentence:\t je ne suis plus votre epoux\n",
      "\n",
      "Model translation:\t je ne suis plus votre mari \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m somewhat dizzy\n",
      "French sentence:\t j ai un peu la tete qui tourne\n",
      "\n",
      "Model translation:\t j ai un peu \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m too busy\n",
      "French sentence:\t je suis trop affaire\n",
      "\n",
      "Model translation:\t je suis trop occupe \n",
      "--------------------------------------------------\n",
      "English sentence:\t i am interested in swimming\n",
      "French sentence:\t la natation m interesse\n",
      "\n",
      "Model translation:\t je natation interesse a la \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m going to work\n",
      "French sentence:\t je vais travailler\n",
      "\n",
      "Model translation:\t je vais travailler \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re restless\n",
      "French sentence:\t tu es agitee\n",
      "\n",
      "Model translation:\t vous es agite \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m not really worried\n",
      "French sentence:\t je ne suis pas vraiment inquiete\n",
      "\n",
      "Model translation:\t je ne suis pas vraiment \n",
      "--------------------------------------------------\n",
      "English sentence:\t we re all impressed\n",
      "French sentence:\t nous sommes toutes impressionnees\n",
      "\n",
      "Model translation:\t nous sommes tous impressionnes \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m not going to tell you again\n",
      "French sentence:\t je ne vais pas vous le repeter\n",
      "\n",
      "Model translation:\t je vais vais pas te repeter \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is very brave\n",
      "French sentence:\t il est fort courageux\n",
      "\n",
      "Model translation:\t il est fort courageux \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re alone aren t you\n",
      "French sentence:\t vous etes seule n est ce pas\n",
      "\n",
      "Model translation:\t vous etes seul n est ce \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m sorry to have kept you waiting\n",
      "French sentence:\t je suis desole de vous avoir fait attendre\n",
      "\n",
      "Model translation:\t je suis desole d avoir avoir attendre \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re sneaky\n",
      "French sentence:\t vous etes sournoises\n",
      "\n",
      "Model translation:\t vous etes sournoises \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re a good liar\n",
      "French sentence:\t tu es bon menteur\n",
      "\n",
      "Model translation:\t vous etes un menteur \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re productive\n",
      "French sentence:\t vous etes productif\n",
      "\n",
      "Model translation:\t vous es productive \n",
      "--------------------------------------------------\n",
      "English sentence:\t i am a gentleman s daughter\n",
      "French sentence:\t je suis la fille d un gentilhomme\n",
      "\n",
      "Model translation:\t je suis un fille d \n",
      "--------------------------------------------------\n",
      "English sentence:\t she is a very kind girl\n",
      "French sentence:\t c est une gentille fille\n",
      "\n",
      "Model translation:\t c est une gentille fille fille \n",
      "--------------------------------------------------\n",
      "English sentence:\t they are christians\n",
      "French sentence:\t ce sont des chretiens\n",
      "\n",
      "Model translation:\t ce sont des \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re hiding something\n",
      "French sentence:\t tu caches quelque chose\n",
      "\n",
      "Model translation:\t tu caches quelque chose \n",
      "--------------------------------------------------\n",
      "English sentence:\t he s raking it in\n",
      "French sentence:\t il s en fourre plein les fouilles\n",
      "\n",
      "Model translation:\t il s en met plein \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m not going to hurt you\n",
      "French sentence:\t je ne vais pas te faire de mal\n",
      "\n",
      "Model translation:\t je ne vais pas te en de \n",
      "--------------------------------------------------\n",
      "English sentence:\t we re not doing this for the money\n",
      "French sentence:\t nous ne le faisons pas pour l argent\n",
      "\n",
      "Model translation:\t nous ne le faisons pour l argent \n",
      "--------------------------------------------------\n",
      "English sentence:\t they are talking about what they will sing\n",
      "French sentence:\t ils parlent de ce qu ils vont chanter\n",
      "\n",
      "Model translation:\t ils parlent des ce qu ils vont des \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re doing it right\n",
      "French sentence:\t tu le fais comme il faut\n",
      "\n",
      "Model translation:\t vous le faites comme \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m horrible with kids\n",
      "French sentence:\t je suis horrible avec les gamins\n",
      "\n",
      "Model translation:\t je suis impressionne avec les \n",
      "--------------------------------------------------\n",
      "English sentence:\t they are always short of money\n",
      "French sentence:\t ils sont toujours a court d argent\n",
      "\n",
      "Model translation:\t ils sont toujours court d argent \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m interfering\n",
      "French sentence:\t je derange\n",
      "\n",
      "Model translation:\t je suis derange \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re the one who trained me\n",
      "French sentence:\t c est vous qui m avez entraine\n",
      "\n",
      "Model translation:\t tu es celui qui m as entrainee \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m late\n",
      "French sentence:\t je suis en retard\n",
      "\n",
      "Model translation:\t je suis en \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is still full of energy\n",
      "French sentence:\t il est encore plein d energie\n",
      "\n",
      "Model translation:\t il est encore plein d energie \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re talkative\n",
      "French sentence:\t vous etes bavarde\n",
      "\n",
      "Model translation:\t vous etes bavard \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re totally ignorant\n",
      "French sentence:\t vous etes completement ignorant\n",
      "\n",
      "Model translation:\t vous etes completement ignorant \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is working on a new novel\n",
      "French sentence:\t il travaille sur un nouveau roman\n",
      "\n",
      "Model translation:\t il travaille sur un nouveau roman \n",
      "--------------------------------------------------\n",
      "English sentence:\t we re a bit late\n",
      "French sentence:\t nous sommes un peu en retard\n",
      "\n",
      "Model translation:\t nous sommes un peu en \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is my type\n",
      "French sentence:\t il est mon genre\n",
      "\n",
      "Model translation:\t c est mon genre \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re very attractive\n",
      "French sentence:\t tu es fort attirante\n",
      "\n",
      "Model translation:\t vous etes fort attirantes \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is a big prankster\n",
      "French sentence:\t c est un grand farceur\n",
      "\n",
      "Model translation:\t c est un grand grand \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re nuts\n",
      "French sentence:\t tu es fou\n",
      "\n",
      "Model translation:\t vous etes dingues \n",
      "--------------------------------------------------\n",
      "English sentence:\t she is always neat and tidy\n",
      "French sentence:\t elle est toujours soignee et ordonnee\n",
      "\n",
      "Model translation:\t elle est toujours et et ordonnee \n",
      "--------------------------------------------------\n",
      "English sentence:\t she s not confident about the future\n",
      "French sentence:\t elle n est pas confiante en l avenir\n",
      "\n",
      "Model translation:\t elle n est pas confiante en l \n",
      "--------------------------------------------------\n",
      "English sentence:\t i am a university student\n",
      "French sentence:\t je suis etudiant a l universite\n",
      "\n",
      "Model translation:\t je suis etudiant etudiant l \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m well acquainted with tom\n",
      "French sentence:\t je connais bien tom\n",
      "\n",
      "Model translation:\t je connais bien tom \n",
      "--------------------------------------------------\n",
      "English sentence:\t she s a fox\n",
      "French sentence:\t c est un renard\n",
      "\n",
      "Model translation:\t c est un renard \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re a terrible dancer\n",
      "French sentence:\t tu es un danseur horrible\n",
      "\n",
      "Model translation:\t vous etes un horrible danseur \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is a good singer\n",
      "French sentence:\t il est bon chanteur\n",
      "\n",
      "Model translation:\t c est un bon chanteur \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m certainly not your friend\n",
      "French sentence:\t je ne suis certainement pas ton amie\n",
      "\n",
      "Model translation:\t je ne suis pas votre ami \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is eager to become famous\n",
      "French sentence:\t il est avide de gloire\n",
      "\n",
      "Model translation:\t il est avide de lui \n",
      "--------------------------------------------------\n",
      "English sentence:\t he s an opera lover\n",
      "French sentence:\t il adore l opera\n",
      "\n",
      "Model translation:\t il adore un opera \n",
      "--------------------------------------------------\n",
      "English sentence:\t he s cruel and heartless\n",
      "French sentence:\t il est cruel et sans c ur\n",
      "\n",
      "Model translation:\t il est cruel et sans \n",
      "--------------------------------------------------\n",
      "English sentence:\t we re in love\n",
      "French sentence:\t nous sommes amoureuses\n",
      "\n",
      "Model translation:\t nous sommes amoureuses \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m a japanese teacher\n",
      "French sentence:\t je suis un professeur de japonais\n",
      "\n",
      "Model translation:\t je suis un l enseignant \n",
      "--------------------------------------------------\n",
      "English sentence:\t i am out of work\n",
      "French sentence:\t je suis sans travail\n",
      "\n",
      "Model translation:\t je suis \n",
      "--------------------------------------------------\n",
      "English sentence:\t you re not my mother\n",
      "French sentence:\t vous n etes pas ma mere\n",
      "\n",
      "Model translation:\t tu n es pas ma \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m not a teacher\n",
      "French sentence:\t je ne suis pas instituteur\n",
      "\n",
      "Model translation:\t je ne suis pas un \n",
      "--------------------------------------------------\n",
      "English sentence:\t he is canadian\n",
      "French sentence:\t il est canadien\n",
      "\n",
      "Model translation:\t il est canadien \n",
      "--------------------------------------------------\n",
      "English sentence:\t i m going to go take a nap\n",
      "French sentence:\t je vais aller faire une sieste\n",
      "\n",
      "Model translation:\t je vais aller faire un somme \n",
      "--------------------------------------------------\n",
      "English sentence:\t we re very satisfied with it\n",
      "French sentence:\t nous en sommes tres satisfaites\n",
      "\n",
      "Model translation:\t nous en sommes tres satisfaites \n",
      "--------------------------------------------------\n",
      "English sentence:\t they re my friends\n",
      "French sentence:\t ils sont mes amis\n",
      "\n",
      "Model translation:\t ils sont mes amis \n",
      "--------------------------------------------------\n",
      "English sentence:\t we are eleven in all\n",
      "French sentence:\t nous sommes onze en tout\n",
      "\n",
      "Model translation:\t nous sommes aux en tous \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def translate(start, end):\n",
    "    for i in range(start, end):\n",
    "        pair = data.idx_pairs[i]\n",
    "        eng_sentence = torch.tensor(pair[0], device=device)\n",
    "        fra_sentence = torch.tensor(pair[1], device=device)\n",
    "\n",
    "        print('English sentence:\\t', ' '.join([eng.index2word[i.item()] for i in eng_sentence[:-1]]))\n",
    "        print('French sentence:\\t', ' '.join([fra.index2word[i.item()] for i in fra_sentence[:-1]]))\n",
    "\n",
    "        # Encode the input language\n",
    "        out, h = encoder(eng_sentence)        \n",
    "        encoder_outputs = torch.zeros(max_length, out.shape[-1], device=device)\n",
    "        encoder_outputs[:out.shape[0], :out.shape[-1]] = out.view(out.shape[0], -1)\n",
    "        \n",
    "        word = torch.tensor([0], device=device) # <SOS>\n",
    "  \n",
    "        translation = []\n",
    "        for j in range(eng_sentence.shape[0]):\n",
    "            x, h = decoder(word, h, encoder_outputs=encoder_outputs)\n",
    "  \n",
    "            word = x.argmax().detach()\n",
    "            translation.append(word.cpu().data.tolist())\n",
    "\n",
    "            if word.item() == 1: # <EOS>\n",
    "                break\n",
    "        print('\\nModel translation:\\t', ' '.join([fra.index2word[i] for i in translation][:-1]), '\\n' + '-'*50)\n",
    "        \n",
    "translate(0, 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
